{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sajjehKqHbl",
        "outputId": "98105210-6e43-4934-e1e8-8198675ae517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (2.4.417150.4580142)\n"
          ]
        }
      ],
      "source": [
        "pip install pymorphy3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsCArs_uuBiI",
        "outputId": "6d527f18-dec8-4247-ad27-9869dc0a2aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers torch sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znWP5LWDsgwJ",
        "outputId": "d923bc0b-bdc5-4f96-eead-9f3c32527e2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "import pymorphy3\n",
        "import re\n",
        "from transformers import pipeline\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "PdOL_BCGrWU5"
      },
      "outputs": [],
      "source": [
        "# 1. Лемматизация и стемминг с помощью pymorphy3\n",
        "def lemmatize_text(text):\n",
        "    tokens = re.findall(r'\\w+', text.lower())  # # первый аргумент из модуя \"re\" означает что мы выбираем целые слова, и преорбразуем их все в нижний регистр\n",
        "    lemmatized_tokens = [morph.parse(token)[0].normal_form for token in tokens]\n",
        "    return \" \".join(lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YjV1enT_r1s6"
      },
      "outputs": [],
      "source": [
        "stemmer = SnowballStemmer(\"russian\")\n",
        "def stem_nltk(text):\n",
        "  words = text.split()  # Простейшая токенизация\n",
        "  res = []\n",
        "  for word in words:\n",
        "    res.append(stemmer.stem(word))\n",
        "  return \" \".join(res) #Вернем текст"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqZHkwrlr7w6",
        "outputId": "7d8f5bfb-1b71-44da-c41c-92d9cb5a3b73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исходныйt: Это пример предложения на русском языке, демонстрирующий удаление стоп-слов.\n",
            "Лемитизированный: это пример предложение на русский язык демонстрировать удаление стоп слово\n",
            "Стемминговый текст: эт пример предложен на русск языке, демонстрир удален стоп-слов.\n"
          ]
        }
      ],
      "source": [
        "text_rus = \"Это пример предложения на русском языке, демонстрирующий удаление стоп-слов.\"\n",
        "#1. Удаляем стоп слова из оригинального текста\n",
        "#2. Лемматизируем и стеммим текст\n",
        "lemmatized_text = lemmatize_text(text_rus)\n",
        "stemmed_text = stem_nltk(text_rus)\n",
        "\n",
        "\n",
        "print(\"Исходныйt:\", text_rus)\n",
        "print(\"Лемитизированный:\", lemmatized_text)\n",
        "print(\"Стемминговый текст:\", stemmed_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz6m3DsfsUUQ",
        "outputId": "cad98898-6e11-4939-bf22-1c3012ce7006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an example of a proposal in Russian showing the deletion of the stop words.\n"
          ]
        }
      ],
      "source": [
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-ru-en\")\n",
        "\n",
        "text_eng = translator(text_rus)[0]['translation_text']\n",
        "print(text_eng)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()  # Указываем язык!\n",
        "\n",
        "def stem_nltk(text):\n",
        "  words = text.split()  # Простейшая токенизация\n",
        "  res = []\n",
        "  for word in words:\n",
        "    res.append(stemmer.stem(word))\n",
        "  return \" \".join(res)\n",
        "\n"
      ],
      "metadata": {
        "id": "_FsrxTaAaMoj"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_nltk(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = word_tokenize(text)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return \" \".join(lemmatized_words)"
      ],
      "metadata": {
        "id": "R8p8ZEBVdQan"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjJnT4aG7FBk",
        "outputId": "19ec0ba6-0460-4c03-d6c8-ac253b50b621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исходный текст: This is an example of a proposal in Russian showing the deletion of the stop words.\n",
            "Лемитизированный текст: This is an example of a proposal in Russian showing the deletion of the stop word .\n",
            "Стеминговый текст: thi is an exampl of a propos in russian show the delet of the stop words.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#2. Лемматизируем и стеммим только отфильтрованный текст\n",
        "lemmatized_text_eng = lemmatize_nltk(text_eng)\n",
        "stemmed_text_eng = stem_nltk(text_eng)\n",
        "\n",
        "\n",
        "print(\"Исходный текст:\", text_eng)\n",
        "print(\"Лемитизированный текст:\", lemmatized_text_eng)\n",
        "print(\"Стеминговый текст:\", stemmed_text_eng)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7nD68N6u-z-",
        "outputId": "da41f129-74b5-4b92-c721-cc8254d8ada7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', 'n', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', ' ', 'o', 'f', ' ', 'a', ' ', 'p', 'r', 'o', 'p', 'o', 's', 'a', 'l', ' ', 'i', 'n', ' ', 'R', 'u', 's', 's', 'i', 'a', 'n', ' ', 's', 'h', 'o', 'w', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'd', 'e', 'l', 'e', 't', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 's', 't', 'o', 'p', ' ', 'w', 'o', 'r', 'd', 's', '.']\n"
          ]
        }
      ],
      "source": [
        "# 2. Функция для токенизации всех символов из ASCII\n",
        "def tokenize(text_eng: str) -> list[str]:\n",
        "    tokens = []\n",
        "    for char in text_eng:\n",
        "        if 0 <= ord(char) <= 127:  #check if char is ascii char\n",
        "            tokens.append(char)\n",
        "    return tokens\n",
        "tok = tokenize(text_eng)\n",
        "print(tok)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "lkq56zclvPLd"
      },
      "outputs": [],
      "source": [
        "def vectorize(tokens: list[str]) -> list[int]:\n",
        "    \"\"\"Vectorizes a list of ASCII characters to a list of their ASCII codes.\"\"\"\n",
        "    vectors = []\n",
        "    for token in tokens:\n",
        "        if len(token) == 1 and 0 <= ord(token) <= 127:\n",
        "            vectors.append(ord(token))\n",
        "    return vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae1ig19eq868",
        "outputId": "8ff33b3c-33a9-4f92-ef75-5dc1e1ce864e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After Lemmatization:\n",
            "Tokenized (first 5): ['t', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', 'n', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', ' ', 'o', 'f', ' ', 'a', ' ', 'p', 'r', 'o', 'p', 'o', 's']\n",
            "Vectorized (first 5): [116, 104, 105, 115, 32, 105, 115, 32, 97, 110, 32, 101, 120, 97, 109, 112, 108, 101, 32, 111, 102, 32, 97, 32, 112, 114, 111, 112, 111, 115]\n",
            "\n",
            "After Stemming:\n",
            "Tokenized (first 5): ['t', 'h', 'i', ' ', 'i', 's', ' ', 'a', 'n', ' ', 'e', 'x', 'a', 'm', 'p', 'l', ' ', 'o', 'f', ' ', 'a', ' ', 'p', 'r', 'o', 'p', 'o', 's', ' ', 'i']\n",
            "Vectorized (first 5): [116, 104, 105, 32, 105, 115, 32, 97, 110, 32, 101, 120, 97, 109, 112, 108, 32, 111, 102, 32, 97, 32, 112, 114, 111, 112, 111, 115, 32, 105]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 4. Токенизация и векторизация текста после лемматизации и стемминга\n",
        "lemmatized_tokens = lemmatize_text(text_eng)\n",
        "stemmed_tokens = stem_nltk(text_eng)\n",
        "\n",
        "\n",
        "tokenized_lemmatized_text = tokenize(\"\".join(lemmatized_tokens))\n",
        "vectorized_lemmatized_text = vectorize(tokenized_lemmatized_text)\n",
        "\n",
        "\n",
        "tokenized_stemmed_text = tokenize(\"\".join(stemmed_tokens))\n",
        "vectorized_stemmed_text = vectorize(tokenized_stemmed_text)\n",
        "\n",
        "print(\"\\nAfter Lemmatization:\")\n",
        "print(\"Tokenized (first 5):\", tokenized_lemmatized_text[:30])\n",
        "print(\"Vectorized (first 5):\", vectorized_lemmatized_text[:30])\n",
        "\n",
        "print(\"\\nAfter Stemming:\")\n",
        "print(\"Tokenized (first 5):\", tokenized_stemmed_text[:30])\n",
        "print(\"Vectorized (first 5):\", vectorized_stemmed_text[:30])\n",
        "\n",
        "# Example Usage with \"Big text\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}