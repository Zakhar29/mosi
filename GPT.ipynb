{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_DSgrUI49C9",
        "outputId": "1b9a5ca3-c95d-40cf-8fd5-d59134de3ee7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNXDRIfP5Cb0",
        "outputId": "708cdcfc-e10a-4839-bec9-196d3b297bdd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (2.4.417150.4580142)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. Импорт библиотек и загрузка данных NLTK**"
      ],
      "metadata": {
        "id": "gpy1wRs-je3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVKPfRTnjcUQ",
        "outputId": "99142183-7f7d-4f74-da82-555f494a9a53"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Класс SimpleTransformer (простая трансформерная модель)**"
      ],
      "metadata": {
        "id": "X8wU2_dykLXo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "T-KbeQVSe8Rr"
      },
      "outputs": [],
      "source": [
        "class SimpleTransformer:\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.W_embed = np.random.randn(self.vocab_size, self.embed_dim) * 0.01\n",
        "        self.W_hidden = np.random.randn(self.embed_dim, self.hidden_dim) * 0.01\n",
        "        self.W_out = np.random.randn(self.hidden_dim, self.vocab_size) * 0.01\n",
        "        self.b_hidden = np.zeros((1, self.hidden_dim))\n",
        "        self.b_out = np.zeros((1, self.vocab_size))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Процесс пропуска через сеть\n",
        "        embed = np.dot(X, self.W_embed)\n",
        "        hidden = np.tanh(np.dot(embed, self.W_hidden) + self.b_hidden)\n",
        "        output = np.dot(hidden, self.W_out) + self.b_out\n",
        "        probs = self.softmax(output)\n",
        "        return probs\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def train_step(self, X, y):\n",
        "        # Обучающий шаг\n",
        "        probs = self.forward(X)\n",
        "        loss = -np.log(probs[0, y] + 1e-9)\n",
        "        grad_output = probs\n",
        "        grad_output[0, y] -= 1\n",
        "        grad_W_out = np.outer(grad_output, X)\n",
        "\n",
        "        grad_hidden = np.dot(grad_output, self.W_out.T) * (1 - np.power(X, 2))\n",
        "        grad_W_hidden = np.outer(grad_hidden, X)\n",
        "\n",
        "        grad_W_embed = np.outer(grad_hidden, X)\n",
        "        return loss\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.forward(X)\n",
        "        return np.argmax(probs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3. Класс TextNeuralNetwork (нейросеть для обработки текста)**"
      ],
      "metadata": {
        "id": "pFZUXyWVkQsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextNeuralNetwork:\n",
        "    def __init__(self, hidden_size=128, seq_length=3, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        Инициализация нейронной сети для текста:\n",
        "        - hidden_size: количество нейронов в скрытом слое (увеличено до 128)\n",
        "        - seq_length: длина последовательности для предсказания\n",
        "        - learning_rate: скорость обучения\n",
        "        \"\"\"\n",
        "        self.hidden_size = hidden_size\n",
        "        self.seq_length = seq_length\n",
        "        self.learning_rate = learning_rate\n",
        "        self.word_to_idx = {}\n",
        "        self.idx_to_word = {}\n",
        "        self.vocab_size = 0\n",
        "        self.loss_history = []\n",
        "    # Альтернатива для русского языка (вместо NLTK)\n",
        "\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        morph = MorphAnalyzer()\n",
        "        tokens = re.findall(r'\\b[а-яё]+\\b', text.lower())\n",
        "        stop_words = {'и', 'в', 'на', 'с', 'по', 'для', 'не', 'а', 'но', 'или'}  # можно расширить\n",
        "        tokens = [morph.parse(word)[0].normal_form for word in tokens if word not in stop_words]\n",
        "\n",
        "        word_counts = Counter(tokens)\n",
        "        vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "        self.word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "        self.idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
        "        self.vocab_size = len(vocab)\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def create_training_data(self, tokens):\n",
        "        \"\"\"Создание обучающих данных (X, y).\"\"\"\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for i in range(len(tokens) - self.seq_length):\n",
        "            seq = tokens[i:i + self.seq_length]\n",
        "            next_word = tokens[i + self.seq_length]\n",
        "\n",
        "            X.append([self.word_to_idx[word] for word in seq])\n",
        "            y.append(self.word_to_idx[next_word])\n",
        "\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Инициализация весов сети.\"\"\"\n",
        "        self.W1 = np.random.randn(self.vocab_size, self.hidden_size) * 0.01\n",
        "        self.b1 = np.zeros((1, self.hidden_size))\n",
        "        self.W2 = np.random.randn(self.hidden_size, self.vocab_size) * 0.01\n",
        "        self.b2 = np.zeros((1, self.vocab_size))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        \"\"\"Функция активации Softmax.\"\"\"\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Прямое распространение.\"\"\"\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = np.tanh(self.z1)\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = self.softmax(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        \"\"\"Вычисление кросс-энтропийной потери.\"\"\"\n",
        "        m = y_true.shape[0]\n",
        "        log_probs = -np.log(y_pred[range(m), y_true] + 1e-9)  # Добавлен эпсилон для численной стабильности\n",
        "        loss = np.sum(log_probs) / m\n",
        "        return loss\n",
        "\n",
        "    def backward(self, X, y_true, y_pred):\n",
        "        \"\"\"Обратное распространение.\"\"\"\n",
        "        m = y_true.shape[0]\n",
        "\n",
        "        # Градиент выходного слоя\n",
        "        grad_z2 = y_pred\n",
        "        grad_z2[range(m), y_true] -= 1\n",
        "        grad_z2 /= m\n",
        "\n",
        "        # Градиенты параметров\n",
        "        grad_W2 = np.dot(self.a1.T, grad_z2)\n",
        "        grad_b2 = np.sum(grad_z2, axis=0, keepdims=True)\n",
        "\n",
        "        # Градиент скрытого слоя\n",
        "        grad_a1 = np.dot(grad_z2, self.W2.T)\n",
        "        grad_z1 = grad_a1 * (1 - np.power(self.a1, 2))  # Производная tanh\n",
        "\n",
        "        grad_W1 = np.dot(X.T, grad_z1)\n",
        "        grad_b1 = np.sum(grad_z1, axis=0, keepdims=True)\n",
        "\n",
        "        return grad_W1, grad_b1, grad_W2, grad_b2\n",
        "\n",
        "    def train(self, text, epochs=10000, verbose=True):\n",
        "        \"\"\"Обучение сети на тексте (увеличено количество эпох до 500).\"\"\"\n",
        "        tokens = self.preprocess_text(text)\n",
        "        X, y = self.create_training_data(tokens)\n",
        "        self.init_weights()\n",
        "\n",
        "        # Преобразование X в one-hot encoding\n",
        "        X_onehot = np.zeros((len(X), self.vocab_size))\n",
        "        for i, seq in enumerate(X):\n",
        "            for word_idx in seq:\n",
        "                X_onehot[i, word_idx] = 1\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Прямое распространение\n",
        "            y_pred = self.forward(X_onehot)\n",
        "\n",
        "            # Вычисление потерь\n",
        "            loss = self.compute_loss(y, y_pred)\n",
        "            self.loss_history.append(loss)\n",
        "\n",
        "            # Обратное распространение\n",
        "            grad_W1, grad_b1, grad_W2, grad_b2 = self.backward(X_onehot, y, y_pred)\n",
        "\n",
        "            # Обновление весов\n",
        "            self.W1 -= self.learning_rate * grad_W1\n",
        "            self.b1 -= self.learning_rate * grad_b1\n",
        "            self.W2 -= self.learning_rate * grad_W2\n",
        "            self.b2 -= self.learning_rate * grad_b2\n",
        "\n",
        "            if verbose and epoch % 50 == 0:  # Вывод каждые 50 эпох\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict_next_word(self, input_seq):\n",
        "        \"\"\"Предсказание следующего слова по последовательности с обработкой неизвестных слов.\"\"\"\n",
        "        seq_indices = []\n",
        "        for word in input_seq:\n",
        "            if word in self.word_to_idx:\n",
        "                seq_indices.append(self.word_to_idx[word])\n",
        "            else:\n",
        "                seq_indices.append(0)  # Используем индекс 0 для неизвестных слов\n",
        "\n",
        "        X = np.zeros((1, self.vocab_size))\n",
        "        for idx in seq_indices:\n",
        "            X[0, idx] = 1\n",
        "\n",
        "        y_pred = self.forward(X)\n",
        "        next_word_idx = np.argmax(y_pred)\n",
        "        return self.idx_to_word[next_word_idx]\n",
        "\n",
        "    def evaluate(self, orders, result):\n",
        "        \"\"\"Оценка модели на тестовых данных.\"\"\"\n",
        "        correct = 0\n",
        "        for i, order in enumerate(orders):\n",
        "            input_seq = re.findall(r'\\b\\w+\\b', order.lower())\n",
        "            if len(input_seq) < self.seq_length:\n",
        "                print(f\"Фраза '{order}' слишком короткая для предсказания\")\n",
        "                continue\n",
        "\n",
        "            input_seq = input_seq[-self.seq_length:]  # Берем последние seq_length слов\n",
        "            try:\n",
        "                predicted = self.predict_next_word(input_seq)\n",
        "                if predicted == result[i]:\n",
        "                    correct += 1\n",
        "                print(f\"'{order}' -> предсказано: '{predicted}', ожидалось: '{result[i]}'\")\n",
        "            except Exception as e:\n",
        "                print(f\"Ошибка при обработке фразы '{order}': {str(e)}\")\n",
        "\n",
        "        accuracy = correct / len(orders)\n",
        "        print(f\"\\nTest Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "ZGSgVTxXjq5_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4**. Тренировочные данные (входной текст и тестовые примеры)**"
      ],
      "metadata": {
        "id": "1wkykqddkfRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Новый текст\n",
        "text = 'На берегу моря стояла девушка. Ветры играли с её волосами, а волны мягко омывали песок. Она смотрела вдаль, где небо сливалось с горизонтом, поглощая всю тревогу и усталость. В её душе царила тишина, она будто почувствовала каждый момент. Эти мгновения были для неё самыми ценными, ведь на этом пляже она всегда чувствовала себя свободной. Вспоминая прошлые дни, она думала о том, как изменилась её жизнь. Старая работа, старые друзья, и эти неведомые горизонты, которые стали её путеводной звездой.'\n",
        "\n",
        "# Новые тестовые данные\n",
        "orders = [\n",
        "    'Девушка стояла на пляже',\n",
        "    'Волны омывали берег',\n",
        "    'Она смотрела в даль',\n",
        "    'Морской воздух был',\n",
        "    'Тишина вокруг неё',\n",
        "    'Она думала о жизни',\n",
        "    'Свобода ощущалась',\n",
        "    'Прошлые дни пролетели'\n",
        "]\n",
        "\n",
        "result = [\n",
        "    'одинокая', 'гладкими', 'мечтая', 'освежающий', 'завораживающая', 'грустные', 'необъятная', 'воспоминания'\n",
        "]\n",
        "\n",
        "# Обучение и оценка\n",
        "model = TextNeuralNetwork()\n",
        "model.train(text, epochs=10000)\n",
        "model.evaluate(orders, result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frCEHvKoj7e4",
        "outputId": "e5e78cd0-9eb5-4ae5-df5b-e4b781b27059"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 4.0775\n",
            "Epoch 50, Loss: 4.0735\n",
            "Epoch 100, Loss: 4.0695\n",
            "Epoch 150, Loss: 4.0656\n",
            "Epoch 200, Loss: 4.0618\n",
            "Epoch 250, Loss: 4.0580\n",
            "Epoch 300, Loss: 4.0543\n",
            "Epoch 350, Loss: 4.0506\n",
            "Epoch 400, Loss: 4.0470\n",
            "Epoch 450, Loss: 4.0434\n",
            "Epoch 500, Loss: 4.0399\n",
            "Epoch 550, Loss: 4.0364\n",
            "Epoch 600, Loss: 4.0330\n",
            "Epoch 650, Loss: 4.0296\n",
            "Epoch 700, Loss: 4.0262\n",
            "Epoch 750, Loss: 4.0229\n",
            "Epoch 800, Loss: 4.0196\n",
            "Epoch 850, Loss: 4.0164\n",
            "Epoch 900, Loss: 4.0132\n",
            "Epoch 950, Loss: 4.0100\n",
            "Epoch 1000, Loss: 4.0069\n",
            "Epoch 1050, Loss: 4.0037\n",
            "Epoch 1100, Loss: 4.0006\n",
            "Epoch 1150, Loss: 3.9975\n",
            "Epoch 1200, Loss: 3.9945\n",
            "Epoch 1250, Loss: 3.9914\n",
            "Epoch 1300, Loss: 3.9884\n",
            "Epoch 1350, Loss: 3.9854\n",
            "Epoch 1400, Loss: 3.9824\n",
            "Epoch 1450, Loss: 3.9794\n",
            "Epoch 1500, Loss: 3.9763\n",
            "Epoch 1550, Loss: 3.9733\n",
            "Epoch 1600, Loss: 3.9703\n",
            "Epoch 1650, Loss: 3.9673\n",
            "Epoch 1700, Loss: 3.9642\n",
            "Epoch 1750, Loss: 3.9611\n",
            "Epoch 1800, Loss: 3.9581\n",
            "Epoch 1850, Loss: 3.9549\n",
            "Epoch 1900, Loss: 3.9518\n",
            "Epoch 1950, Loss: 3.9486\n",
            "Epoch 2000, Loss: 3.9454\n",
            "Epoch 2050, Loss: 3.9421\n",
            "Epoch 2100, Loss: 3.9388\n",
            "Epoch 2150, Loss: 3.9354\n",
            "Epoch 2200, Loss: 3.9319\n",
            "Epoch 2250, Loss: 3.9284\n",
            "Epoch 2300, Loss: 3.9248\n",
            "Epoch 2350, Loss: 3.9211\n",
            "Epoch 2400, Loss: 3.9173\n",
            "Epoch 2450, Loss: 3.9134\n",
            "Epoch 2500, Loss: 3.9094\n",
            "Epoch 2550, Loss: 3.9053\n",
            "Epoch 2600, Loss: 3.9011\n",
            "Epoch 2650, Loss: 3.8968\n",
            "Epoch 2700, Loss: 3.8923\n",
            "Epoch 2750, Loss: 3.8876\n",
            "Epoch 2800, Loss: 3.8828\n",
            "Epoch 2850, Loss: 3.8779\n",
            "Epoch 2900, Loss: 3.8727\n",
            "Epoch 2950, Loss: 3.8674\n",
            "Epoch 3000, Loss: 3.8618\n",
            "Epoch 3050, Loss: 3.8561\n",
            "Epoch 3100, Loss: 3.8501\n",
            "Epoch 3150, Loss: 3.8439\n",
            "Epoch 3200, Loss: 3.8374\n",
            "Epoch 3250, Loss: 3.8307\n",
            "Epoch 3300, Loss: 3.8237\n",
            "Epoch 3350, Loss: 3.8164\n",
            "Epoch 3400, Loss: 3.8088\n",
            "Epoch 3450, Loss: 3.8008\n",
            "Epoch 3500, Loss: 3.7926\n",
            "Epoch 3550, Loss: 3.7839\n",
            "Epoch 3600, Loss: 3.7750\n",
            "Epoch 3650, Loss: 3.7656\n",
            "Epoch 3700, Loss: 3.7559\n",
            "Epoch 3750, Loss: 3.7457\n",
            "Epoch 3800, Loss: 3.7351\n",
            "Epoch 3850, Loss: 3.7241\n",
            "Epoch 3900, Loss: 3.7127\n",
            "Epoch 3950, Loss: 3.7007\n",
            "Epoch 4000, Loss: 3.6883\n",
            "Epoch 4050, Loss: 3.6755\n",
            "Epoch 4100, Loss: 3.6621\n",
            "Epoch 4150, Loss: 3.6482\n",
            "Epoch 4200, Loss: 3.6338\n",
            "Epoch 4250, Loss: 3.6189\n",
            "Epoch 4300, Loss: 3.6034\n",
            "Epoch 4350, Loss: 3.5874\n",
            "Epoch 4400, Loss: 3.5709\n",
            "Epoch 4450, Loss: 3.5538\n",
            "Epoch 4500, Loss: 3.5362\n",
            "Epoch 4550, Loss: 3.5181\n",
            "Epoch 4600, Loss: 3.4994\n",
            "Epoch 4650, Loss: 3.4801\n",
            "Epoch 4700, Loss: 3.4603\n",
            "Epoch 4750, Loss: 3.4400\n",
            "Epoch 4800, Loss: 3.4192\n",
            "Epoch 4850, Loss: 3.3979\n",
            "Epoch 4900, Loss: 3.3761\n",
            "Epoch 4950, Loss: 3.3538\n",
            "Epoch 5000, Loss: 3.3310\n",
            "Epoch 5050, Loss: 3.3078\n",
            "Epoch 5100, Loss: 3.2841\n",
            "Epoch 5150, Loss: 3.2600\n",
            "Epoch 5200, Loss: 3.2356\n",
            "Epoch 5250, Loss: 3.2107\n",
            "Epoch 5300, Loss: 3.1855\n",
            "Epoch 5350, Loss: 3.1600\n",
            "Epoch 5400, Loss: 3.1341\n",
            "Epoch 5450, Loss: 3.1080\n",
            "Epoch 5500, Loss: 3.0816\n",
            "Epoch 5550, Loss: 3.0550\n",
            "Epoch 5600, Loss: 3.0281\n",
            "Epoch 5650, Loss: 3.0010\n",
            "Epoch 5700, Loss: 2.9738\n",
            "Epoch 5750, Loss: 2.9464\n",
            "Epoch 5800, Loss: 2.9189\n",
            "Epoch 5850, Loss: 2.8912\n",
            "Epoch 5900, Loss: 2.8634\n",
            "Epoch 5950, Loss: 2.8355\n",
            "Epoch 6000, Loss: 2.8076\n",
            "Epoch 6050, Loss: 2.7795\n",
            "Epoch 6100, Loss: 2.7514\n",
            "Epoch 6150, Loss: 2.7233\n",
            "Epoch 6200, Loss: 2.6951\n",
            "Epoch 6250, Loss: 2.6670\n",
            "Epoch 6300, Loss: 2.6388\n",
            "Epoch 6350, Loss: 2.6105\n",
            "Epoch 6400, Loss: 2.5823\n",
            "Epoch 6450, Loss: 2.5541\n",
            "Epoch 6500, Loss: 2.5260\n",
            "Epoch 6550, Loss: 2.4978\n",
            "Epoch 6600, Loss: 2.4697\n",
            "Epoch 6650, Loss: 2.4416\n",
            "Epoch 6700, Loss: 2.4135\n",
            "Epoch 6750, Loss: 2.3855\n",
            "Epoch 6800, Loss: 2.3575\n",
            "Epoch 6850, Loss: 2.3296\n",
            "Epoch 6900, Loss: 2.3018\n",
            "Epoch 6950, Loss: 2.2740\n",
            "Epoch 7000, Loss: 2.2463\n",
            "Epoch 7050, Loss: 2.2187\n",
            "Epoch 7100, Loss: 2.1911\n",
            "Epoch 7150, Loss: 2.1636\n",
            "Epoch 7200, Loss: 2.1363\n",
            "Epoch 7250, Loss: 2.1090\n",
            "Epoch 7300, Loss: 2.0819\n",
            "Epoch 7350, Loss: 2.0548\n",
            "Epoch 7400, Loss: 2.0279\n",
            "Epoch 7450, Loss: 2.0012\n",
            "Epoch 7500, Loss: 1.9746\n",
            "Epoch 7550, Loss: 1.9481\n",
            "Epoch 7600, Loss: 1.9219\n",
            "Epoch 7650, Loss: 1.8958\n",
            "Epoch 7700, Loss: 1.8699\n",
            "Epoch 7750, Loss: 1.8442\n",
            "Epoch 7800, Loss: 1.8188\n",
            "Epoch 7850, Loss: 1.7936\n",
            "Epoch 7900, Loss: 1.7686\n",
            "Epoch 7950, Loss: 1.7439\n",
            "Epoch 8000, Loss: 1.7195\n",
            "Epoch 8050, Loss: 1.6953\n",
            "Epoch 8100, Loss: 1.6715\n",
            "Epoch 8150, Loss: 1.6479\n",
            "Epoch 8200, Loss: 1.6247\n",
            "Epoch 8250, Loss: 1.6018\n",
            "Epoch 8300, Loss: 1.5792\n",
            "Epoch 8350, Loss: 1.5569\n",
            "Epoch 8400, Loss: 1.5350\n",
            "Epoch 8450, Loss: 1.5135\n",
            "Epoch 8500, Loss: 1.4923\n",
            "Epoch 8550, Loss: 1.4714\n",
            "Epoch 8600, Loss: 1.4509\n",
            "Epoch 8650, Loss: 1.4308\n",
            "Epoch 8700, Loss: 1.4110\n",
            "Epoch 8750, Loss: 1.3916\n",
            "Epoch 8800, Loss: 1.3725\n",
            "Epoch 8850, Loss: 1.3537\n",
            "Epoch 8900, Loss: 1.3353\n",
            "Epoch 8950, Loss: 1.3173\n",
            "Epoch 9000, Loss: 1.2996\n",
            "Epoch 9050, Loss: 1.2822\n",
            "Epoch 9100, Loss: 1.2651\n",
            "Epoch 9150, Loss: 1.2483\n",
            "Epoch 9200, Loss: 1.2319\n",
            "Epoch 9250, Loss: 1.2158\n",
            "Epoch 9300, Loss: 1.1999\n",
            "Epoch 9350, Loss: 1.1844\n",
            "Epoch 9400, Loss: 1.1691\n",
            "Epoch 9450, Loss: 1.1541\n",
            "Epoch 9500, Loss: 1.1394\n",
            "Epoch 9550, Loss: 1.1250\n",
            "Epoch 9600, Loss: 1.1108\n",
            "Epoch 9650, Loss: 1.0969\n",
            "Epoch 9700, Loss: 1.0832\n",
            "Epoch 9750, Loss: 1.0697\n",
            "Epoch 9800, Loss: 1.0565\n",
            "Epoch 9850, Loss: 1.0435\n",
            "Epoch 9900, Loss: 1.0307\n",
            "Epoch 9950, Loss: 1.0181\n",
            "'Девушка стояла на пляже' -> предсказано: 'каждый', ожидалось: 'одинокая'\n",
            "'Волны омывали берег' -> предсказано: 'себя', ожидалось: 'гладкими'\n",
            "'Она смотрела в даль' -> предсказано: 'каждый', ожидалось: 'мечтая'\n",
            "'Морской воздух был' -> предсказано: 'каждый', ожидалось: 'освежающий'\n",
            "'Тишина вокруг неё' -> предсказано: 'почувствовать', ожидалось: 'завораживающая'\n",
            "'Она думала о жизни' -> предсказано: 'тот', ожидалось: 'грустные'\n",
            "Фраза 'Свобода ощущалась' слишком короткая для предсказания\n",
            "'Прошлые дни пролетели' -> предсказано: 'каждый', ожидалось: 'воспоминания'\n",
            "\n",
            "Test Accuracy: 0.00%\n"
          ]
        }
      ]
    }
  ]
}